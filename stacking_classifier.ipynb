{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Load your dataset\n",
    "# df = pd.read_csv(\"data\\data.csv\") \n",
    "\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# X = df.drop(['label', 'filename'], axis=1)\n",
    "# y = df['label']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define your base classifiers\n",
    "# base_classifiers = [\n",
    "#     ('rf', RandomForestClassifier(random_state=42)),\n",
    "#     ('svc', SVC(probability=True, random_state=42)),\n",
    "#     ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')),\n",
    "#     ('mlp', MLPClassifier(random_state=42)),\n",
    "#     ('knn', KNeighborsClassifier()),\n",
    "#     ('log_clf', LogisticRegression(random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Meta-classifier\n",
    "# meta_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# # Initialize the Stacking Classifier\n",
    "# stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier, cv=5)\n",
    "\n",
    "# # Parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'rf__n_estimators': [100, 200, 300],\n",
    "#     'rf__max_depth': [10, None, 5],\n",
    "#     'svc__C': [0.1, 1, 10],\n",
    "#     'xgb__n_estimators': [50, 100, 150],\n",
    "#     'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "#     'mlp__max_iter': [500, 1000, 1500],\n",
    "#     'knn__n_neighbors': [3, 5, 7],\n",
    "#     'log_clf__C': [0.1, 1, 10],\n",
    "#     'final_estimator__C': [0.1, 1, 10]\n",
    "# }\n",
    "\n",
    "# # Setup GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=stacking_clf, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# # Perform the search\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Best parameters and score\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred = grid_search.predict(X_test_scaled)\n",
    "# print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "# print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"data\\data.csv\") \n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "X = df.drop(['label', 'filename'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint as sp_randint\n",
    "# from scipy.stats import uniform\n",
    "\n",
    "# # Assuming the imports and data preparation steps are done as before\n",
    "\n",
    "# # Base classifiers with example classifiers\n",
    "# base_classifiers = [\n",
    "#     ('rf', RandomForestClassifier(random_state=42)),\n",
    "#     ('svc', SVC(probability=True, random_state=42)),\n",
    "#     ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')),\n",
    "#     ('mlp', MLPClassifier(random_state=42)),\n",
    "#     ('knn', KNeighborsClassifier()),\n",
    "#     ('log_clf', LogisticRegression(random_state=42))\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Meta-classifier\n",
    "# meta_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# # Initialize the Stacking Classifier\n",
    "# stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier, cv=5)\n",
    "\n",
    "# # Parameter distributions\n",
    "# param_dist = {\n",
    "#     'rf__n_estimators': sp_randint(100, 500),  # Number of trees in random forest\n",
    "#     'rf__max_depth': [None, 10, 20, 30],  # Maximum depth of trees\n",
    "#     'svc__C': uniform(0.1, 10),  # Penalty parameter C of the SVC\n",
    "#     'xgb__n_estimators': sp_randint(50, 150),  # Number of gradient boosted trees\n",
    "#     'xgb__learning_rate': uniform(0.01, 0.2),  # Boosting learning rate\n",
    "#     'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # MLP architecture\n",
    "#     'mlp__alpha': uniform(0.0001, 0.001),  # L2 penalty (regularization term) parameter\n",
    "#     'knn__n_neighbors': sp_randint(3, 10),  # Number of neighbors to use\n",
    "#     'log_clf__C': uniform(0.1, 10),  # Inverse of regularization strength in LogisticRegression\n",
    "#     'final_estimator__C': uniform(0.1, 10),  # Regularization in the final meta-classifier\n",
    "# }\n",
    "\n",
    "\n",
    "# # Initialize RandomizedSearchCV with the Stacking Classifier and parameter distribution\n",
    "# random_search = RandomizedSearchCV(stacking_clf, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Perform the search\n",
    "# random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Best parameters and score\n",
    "# print(\"Best parameters found: \", random_search.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(random_search.best_score_))\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# y_pred = random_search.predict(X_test_scaled)\n",
    "# print(\"Accuracy on test set: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "# print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bryso\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.75      0.75      0.75        20\n",
      "   classical       0.87      1.00      0.93        13\n",
      "     country       0.59      0.74      0.66        27\n",
      "       disco       0.60      0.43      0.50        21\n",
      "      hiphop       0.52      0.73      0.61        15\n",
      "        jazz       0.75      0.68      0.71        22\n",
      "       metal       0.80      0.80      0.80        25\n",
      "         pop       0.86      0.92      0.89        13\n",
      "      reggae       0.63      0.52      0.57        23\n",
      "        rock       0.35      0.29      0.32        21\n",
      "\n",
      "    accuracy                           0.67       200\n",
      "   macro avg       0.67      0.69      0.67       200\n",
      "weighted avg       0.66      0.67      0.66       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Reinitialize the classifiers with the best hyperparameters\n",
    "rf_best = RandomForestClassifier(n_estimators=317, max_depth=10, random_state=42)\n",
    "svc_best = SVC(C=7.003948286293653, probability=True, random_state=42)\n",
    "xgb_best = XGBClassifier(n_estimators=96, learning_rate=0.01786242796821979, use_label_encoder=False, eval_metric='mlogloss')\n",
    "mlp_best = MLPClassifier(alpha=0.0009942172628959257, hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3)\n",
    "log_clf_best = LogisticRegression(C=3.659914842719861, max_iter=5000, random_state=42)\n",
    "\n",
    "# Meta-classifier with the best hyperparameters\n",
    "meta_classifier_best = LogisticRegression(C=1.3063587110060082, random_state=42)\n",
    "\n",
    "# Create the Stacking Classifier with the best hyperparameters\n",
    "stacking_clf_best = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_best),\n",
    "        ('svc', svc_best),\n",
    "        ('xgb', xgb_best),\n",
    "        ('mlp', mlp_best),\n",
    "        ('knn', knn_best),\n",
    "        ('log_clf', log_clf_best)\n",
    "    ],\n",
    "    final_estimator=meta_classifier_best,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Assuming X_train_scaled, y_train, X_test_scaled, y_test are already defined and prepared\n",
    "\n",
    "# Fit the Stacking Classifier with the best hyperparameters\n",
    "stacking_clf_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred_best = stacking_clf_best.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best)}\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import jsonpickle\n",
    "\n",
    "# Correct the path to the new dataset\n",
    "path = 'data/features_3_sec.csv'  # Adjusted to use forward slashes\n",
    "\n",
    "# Load the new dataset\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Define features and labels\n",
    "X = df.drop([\"label\", \"filename\", 'length'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "scaler_json = jsonpickle.encode(scaler)\n",
    "\n",
    "# Save the JSON string to a file\n",
    "with open(\"scaler.json\", \"w\") as file:\n",
    "    file.write(scaler_json)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.944944944944945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.96      0.94      0.95       208\n",
      "   classical       0.96      0.96      0.96       203\n",
      "     country       0.90      0.90      0.90       186\n",
      "       disco       0.96      0.94      0.95       199\n",
      "      hiphop       0.96      0.95      0.96       218\n",
      "        jazz       0.90      0.95      0.93       192\n",
      "       metal       0.98      0.99      0.98       204\n",
      "         pop       0.94      0.97      0.95       180\n",
      "      reggae       0.96      0.94      0.95       211\n",
      "        rock       0.93      0.91      0.92       197\n",
      "\n",
      "    accuracy                           0.94      1998\n",
      "   macro avg       0.94      0.94      0.94      1998\n",
      "weighted avg       0.95      0.94      0.94      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base classifiers with example classifiers\n",
    "base_classifiers = [\n",
    "    (\"rf\", RandomForestClassifier(random_state=42)),\n",
    "    (\"svc\", SVC(probability=True, random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(eval_metric=\"mlogloss\", random_state=42)),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        MLPClassifier(\n",
    "            alpha=0.0009942172628959257,\n",
    "            hidden_layer_sizes=(50,),\n",
    "            max_iter=2000,\n",
    "            random_state=42,\n",
    "        ),\n",
    "    ),\n",
    "    (\"knn\", KNeighborsClassifier()),\n",
    "    (\n",
    "        \"log_clf\",\n",
    "        LogisticRegression(\n",
    "            C=3.659914842719861, max_iter=5000, solver=\"saga\", random_state=42\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Meta-classifier\n",
    "meta_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize the Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_classifiers, final_estimator=meta_classifier, cv=5\n",
    ")\n",
    "\n",
    "# Fit the Stacking Classifier on the scaled training data\n",
    "stacking_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the scaled test set and evaluate\n",
    "y_pred = stacking_clf.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# Save the model to disk\n",
    "model_filename = \"stacking_classifier_model.joblib\"\n",
    "dump(stacking_clf, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[16:41:14] C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:1220: Check failed: fp.Read(&header[0], header.size()) == serialisation_header_.size() (0 vs. 14) : ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_json \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Deserialize the JSON string back into a model object\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mjsonpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use the loaded model to make predictions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m y_pred_loaded \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:85\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(string, backend, context, keys, reset, safe, classes, v1_decode, on_missing)\u001b[0m\n\u001b[0;32m     77\u001b[0m context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;129;01mor\u001b[39;00m Unpickler(\n\u001b[0;32m     78\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[0;32m     79\u001b[0m     backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     on_missing\u001b[38;5;241m=\u001b[39mon_missing,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m data \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mdecode(string)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:355\u001b[0m, in \u001b[0;36mUnpickler.restore\u001b[1;34m(self, obj, reset, classes)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classes:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_classes(classes)\n\u001b[1;32m--> 355\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap_proxies()\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:756\u001b[0m, in \u001b[0;36mUnpickler._restore_object\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkref(obj)\n\u001b[1;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:732\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance\u001b[1;34m(self, obj, cls, class_name)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n\u001b[1;32m--> 732\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _safe_hasattr(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_factory\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    735\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory, _Proxy\n\u001b[0;32m    736\u001b[0m ):\n\u001b[0;32m    737\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mdefault_factory\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:678\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance_variables\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    675\u001b[0m             instance\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore(v))\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tag(obj, tags\u001b[38;5;241m.\u001b[39mSTATE):\n\u001b[1;32m--> 678\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:637\u001b[0m, in \u001b[0;36mUnpickler._restore_state\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_restore_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, instance):\n\u001b[1;32m--> 637\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m     has_slots \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    640\u001b[0m     )\n\u001b[0;32m    641\u001b[0m     has_slots_and_dict \u001b[38;5;241m=\u001b[39m has_slots \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:808\u001b[0m, in \u001b[0;36mUnpickler._restore_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    806\u001b[0m             str_k \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mappend(str_k)\n\u001b[1;32m--> 808\u001b[0m         data[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:421\u001b[0m, in \u001b[0;36mUnpickler._restore_list\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    419\u001b[0m parent \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkref(parent)\n\u001b[1;32m--> 421\u001b[0m children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    422\u001b[0m parent\u001b[38;5;241m.\u001b[39mextend(children)\n\u001b[0;32m    423\u001b[0m method \u001b[38;5;241m=\u001b[39m _obj_setvalue\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:421\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    419\u001b[0m parent \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkref(parent)\n\u001b[1;32m--> 421\u001b[0m children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    422\u001b[0m parent\u001b[38;5;241m.\u001b[39mextend(children)\n\u001b[0;32m    423\u001b[0m method \u001b[38;5;241m=\u001b[39m _obj_setvalue\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:756\u001b[0m, in \u001b[0;36mUnpickler._restore_object\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkref(obj)\n\u001b[1;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:732\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance\u001b[1;34m(self, obj, cls, class_name)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n\u001b[1;32m--> 732\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _safe_hasattr(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_factory\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    735\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory, _Proxy\n\u001b[0;32m    736\u001b[0m ):\n\u001b[0;32m    737\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mdefault_factory\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:678\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance_variables\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    675\u001b[0m             instance\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore(v))\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tag(obj, tags\u001b[38;5;241m.\u001b[39mSTATE):\n\u001b[1;32m--> 678\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:637\u001b[0m, in \u001b[0;36mUnpickler._restore_state\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_restore_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, instance):\n\u001b[1;32m--> 637\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m     has_slots \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    640\u001b[0m     )\n\u001b[0;32m    641\u001b[0m     has_slots_and_dict \u001b[38;5;241m=\u001b[39m has_slots \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:808\u001b[0m, in \u001b[0;36mUnpickler._restore_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    806\u001b[0m             str_k \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mappend(str_k)\n\u001b[1;32m--> 808\u001b[0m         data[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namestack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:337\u001b[0m, in \u001b[0;36mUnpickler._restore\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_tags(obj)\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:756\u001b[0m, in \u001b[0;36mUnpickler._restore_object\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkref(obj)\n\u001b[1;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:732\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance\u001b[1;34m(self, obj, cls, class_name)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n\u001b[1;32m--> 732\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_object_instance_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _safe_hasattr(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_factory\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    735\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory, _Proxy\n\u001b[0;32m    736\u001b[0m ):\n\u001b[0;32m    737\u001b[0m     instance\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mdefault_factory\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:678\u001b[0m, in \u001b[0;36mUnpickler._restore_object_instance_variables\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    675\u001b[0m             instance\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore(v))\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tag(obj, tags\u001b[38;5;241m.\u001b[39mSTATE):\n\u001b[1;32m--> 678\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\jsonpickle\\unpickler.py:643\u001b[0m, in \u001b[0;36mUnpickler._restore_state\u001b[1;34m(self, obj, instance)\u001b[0m\n\u001b[0;32m    641\u001b[0m has_slots_and_dict \u001b[38;5;241m=\u001b[39m has_slots \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 643\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__setstate__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# implements described default handling\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# of state for object with instance dict\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;66;03m# and no slots\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restore_from_dict(state, instance, ignorereserved\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\xgboost\\core.py:1681\u001b[0m, in \u001b[0;36mBooster.__setstate__\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     length \u001b[38;5;241m=\u001b[39m c_bst_ulong(\u001b[38;5;28mlen\u001b[39m(buf))\n\u001b[0;32m   1680\u001b[0m     ptr \u001b[38;5;241m=\u001b[39m (ctypes\u001b[38;5;241m.\u001b[39mc_char \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf))\u001b[38;5;241m.\u001b[39mfrom_buffer(buf)\n\u001b[1;32m-> 1681\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUnserializeFromBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1683\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhandle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m handle\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(state)\n",
      "File \u001b[1;32mc:\\Users\\gmspr\\anaconda3\\envs\\stats\\lib\\site-packages\\xgboost\\core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [16:41:14] C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:1220: Check failed: fp.Read(&header[0], header.size()) == serialisation_header_.size() (0 vs. 14) : "
     ]
    }
   ],
   "source": [
    "# Load the model from disk\n",
    "model_filename = 'stacking_classifier_model.joblib'\n",
    "loaded_model = load(model_filename)\n",
    "print(\"Model loaded\")\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "y_pred_loaded = loaded_model.predict(X_test_scaled)\n",
    "print(f\"Accuracy of loaded model: {accuracy_score(y_test, y_pred_loaded)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the path to the new dataset\n",
    "import pandas as pd\n",
    "\n",
    "path = \"data/features_3_sec.csv\"  # Adjusted to use forward slashes\n",
    "\n",
    "# Load the new dataset\n",
    "df = pd.read_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
